{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group : Ankitha Kumari Moodukudru, Manisha Sharma, Rishabh Agrawal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen as uReq\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import math\n",
    "import nltk\n",
    "import os\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from collections import OrderedDict\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from difflib import get_close_matches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   GO = '\\033[0;30;43m'\n",
    "   END = '\\033[0m'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = string.punctuation\n",
    "remove = remove.replace(\"-\", \"\")\n",
    "pattern = r\"[{}]\".format(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "levendata = {}\n",
    "def levenshtein(s, t):\n",
    "    if s == \"\":\n",
    "        return len(t)\n",
    "    if t == \"\":\n",
    "        return len(s)\n",
    "    cost = 0 if s[-1] == t[-1] else 1\n",
    "       \n",
    "    i1 = (s[:-1], t)\n",
    "    if not i1 in levendata:\n",
    "        levendata[i1] = levenshtein(*i1)\n",
    "    i2 = (s, t[:-1])\n",
    "    if not i2 in levendata:\n",
    "        levendata[i2] = levenshtein(*i2)\n",
    "    i3 = (s[:-1], t[:-1])\n",
    "    if not i3 in levendata:\n",
    "        levendata[i3] = levenshtein(*i3)\n",
    "    res = min([levendata[i1]+1, levendata[i2]+1, levendata[i3]+cost])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getallsimilar(query, diction, n = 6):\n",
    "    a = {}\n",
    "    for i in diction:\n",
    "        a[i]= levenshtein(query, i)\n",
    "    a = sorted(a.items(), key=operator.itemgetter(1))\n",
    "    a = pd.DataFrame(a)\n",
    "    return np.array(a[0].head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemquery(query):\n",
    "    query = query.split(\" \")\n",
    "    ps = PorterStemmer()\n",
    "    newquery = []\n",
    "    for q in query:\n",
    "        newquery.append(ps.stem(q))\n",
    "    return \" \".join(newquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readstemcorpus(location, typeoffile):\n",
    "    with open(location, \"r\", encoding=\"utf-8\") as myfile:\n",
    "        stemfile=myfile.read().replace('\\n', ' ')\n",
    "        myfile.close()\n",
    "    stemfile = stemfile.split(\"#\")\n",
    "    stemfile.remove(\"\")\n",
    "    stemfiledict = {}\n",
    "    for i in stemfile:\n",
    "        main = i.split(\" \")\n",
    "        while '' in main:\n",
    "            main.remove('')\n",
    "        strin = []\n",
    "        if int(main[0]) < 10:\n",
    "            name = \"CACM-\" + \"0\"*3 + str(main[0])\n",
    "        elif int(main[0]) < 100:\n",
    "            name = \"CACM-\" + \"0\"*2 + str(main[0])\n",
    "        elif int(main[0]) < 1000:\n",
    "            name = \"CACM-\" + \"0\"*1 + str(main[0])\n",
    "        else:\n",
    "            name = \"CACM-\" + str(main[0])\n",
    "            \n",
    "        for j in range(1,len(main)):\n",
    "            if (main[j] == \"pm\" or main[j] == \"am\"):\n",
    "                break\n",
    "            strin.append(main[j])\n",
    "        stemfiledict[name] = \" \".join(strin)\n",
    "    unigramdict = defaultdict(list)\n",
    "    documentandlen = {}\n",
    "    for i,j in stemfiledict.items():\n",
    "        tokens = word_tokenize(j)\n",
    "        token=[token.lower() for token in tokens if (re.match(r'^[a-zA-Z0-9][ A-Za-z0-9-]*$', token))]\n",
    "        documentandlen[i] = len(token)\n",
    "        countwordsdoc = 0\n",
    "        for j in range(len(token)):\n",
    "            countwordsdoc +=1\n",
    "            string = token[j]\n",
    "            if string in unigramdict:\n",
    "                tempi = str(i)\n",
    "                if tempi in  unigramdict[string][0]:\n",
    "                    unigramdict[string][0][tempi] += 1\n",
    "                else:\n",
    "                    unigramdict[string][0][tempi] = 1\n",
    "\n",
    "            else:\n",
    "                tempi = str(i)\n",
    "                b = {}\n",
    "                unigramdict[string].append(b)\n",
    "                unigramdict[string][0][tempi] = 1\n",
    "\n",
    "\n",
    "        documentandlen[i] = countwordsdoc\n",
    "    return unigramdict, documentandlen    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psuedorel(query, location, typeoffile, stopwords = []):\n",
    "    query = query.lower()\n",
    "    psuedodict , psuedolen = fetchunigramdict(location, typeoffile, filelist = [], stopwords = stopwords)\n",
    "    an1 = BM25(query, documentandlen = psuedolen, uniindex = psuedodict)\n",
    "    an11 = sorted(an1.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    an11 = pd.DataFrame(an11)\n",
    "    an11 = an11.head(100)\n",
    "    x = np.array(an11.head(100)[0])\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + \".html\"\n",
    "\n",
    "    for i in range(10):\n",
    "        psuedodict , psuedolen = fetchunigramdict(location, typeoffile, typ = 1, filelist = np.array(x), stopwords = stopwords)\n",
    "        an1 = BM25(query, documentandlen = psuedolen, uniindex = psuedodict)\n",
    "        an11 = sorted(an1.items(), key=operator.itemgetter(1), reverse = True)\n",
    "        an11 = pd.DataFrame(an11)\n",
    "        an11 = an11.head(100)\n",
    "        x = np.array(an11.head(100)[0])\n",
    "        for i in range(len(x)):\n",
    "            x[i] = x[i] + \".html\"\n",
    "        termcount = {}\n",
    "        for i,j in psuedodict.items():\n",
    "                sum1 = 0\n",
    "                for j,k in psuedodict[i][0].items():\n",
    "                    sum1 += k\n",
    "                termcount[i] = sum1\n",
    "        termc = sorted(termcount.items(), key=operator.itemgetter(1), reverse = True)\n",
    "        termc = pd.DataFrame(termc)    \n",
    "        for j in range(len(termc)):\n",
    "            kc = 0\n",
    "            qst = query.split(\" \")\n",
    "            for q in qst:\n",
    "                if (q == termc[0][j]):\n",
    "                    kc = 1\n",
    "                    break\n",
    "            if kc == 1:\n",
    "                continue\n",
    "            if kc == 0:\n",
    "                query = query + \" \" + termc[0][j]\n",
    "                break\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(query, documentandlen, uniindex):\n",
    "    a = {}     \n",
    "    for i,j in documentandlen.items():\n",
    "        a[i] = 0\n",
    "    query = tokenizequery(query)\n",
    "    q = {}\n",
    "    for i in query:\n",
    "        i = i.lower()\n",
    "        if i in q:\n",
    "            q[i] += 1\n",
    "        else:\n",
    "             q[i] = 1\n",
    "    \n",
    "    for i in query:\n",
    "        if i not in uniindex:\n",
    "            continue\n",
    "        for k,l in documentandlen.items():\n",
    "            if k not in uniindex[i][0]:\n",
    "                continue\n",
    "            tf = uniindex[i][0][k] / documentandlen[k]\n",
    "            idf = np.log(len(documentandlen) / len(uniindex[i][0]))\n",
    "            a[k] += tf*idf\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querylikeDirichlet(query, documentandlen, uniindex):\n",
    "    a = {}     \n",
    "    totaldoc = 0\n",
    "    \n",
    "    for i,j in documentandlen.items():\n",
    "        a[i] = 1\n",
    "        totaldoc += j\n",
    "        \n",
    "    query = tokenizequery(query)\n",
    "    q = {}\n",
    "    for i in query:\n",
    "        i = i.lower()\n",
    "        if i in q:\n",
    "            q[i] += 1\n",
    "        else:\n",
    "             q[i] = 1\n",
    "    termcount = {}\n",
    "    \n",
    "    for i,j in uniindex.items():\n",
    "        sum1 = 0\n",
    "        for j,k in uniindex[i][0].items():\n",
    "            sum1 += k\n",
    "        termcount[i] = sum1\n",
    "    \n",
    "    for i in query:\n",
    "        if i not in uniindex:\n",
    "            continue\n",
    "        for k,l in documentandlen.items():\n",
    "            mu = sum(documentandlen.values()) / len(documentandlen)\n",
    "            \n",
    "            if k in uniindex[i][0]:\n",
    "                top = uniindex[i][0][k] + mu * (termcount[i]/sum(documentandlen.values()))\n",
    "                down = documentandlen[k] + mu\n",
    "                a[k] += np.log(top/down)\n",
    "            else:\n",
    "                top = 0 + mu * (termcount[i]/sum(documentandlen.values()))\n",
    "                down = documentandlen[k] + mu\n",
    "                a[k] += np.log(top/down)\n",
    "            \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(query, documentandlen, uniindex, k1 = 1.2, k2 = 100, b = 0.75, R = 0):\n",
    "    a = {}\n",
    "    for i,j in documentandlen.items():\n",
    "        a[i] = 0\n",
    "    \n",
    "    query = tokenizequery(query)\n",
    "    q = {}\n",
    "    \n",
    "    N = len(documentandlen)\n",
    "        \n",
    "    avdl = 0\n",
    "    for i,j in documentandlen.items():\n",
    "        avdl += j\n",
    "    avdl /= len(documentandlen) \n",
    "        \n",
    "    for i in query:\n",
    "        i = i.lower()\n",
    "        if i in q:\n",
    "            q[i] += 1\n",
    "        else:\n",
    "             q[i] = 1\n",
    "    \n",
    "    for i,j in q.items():\n",
    "        if i not in uniindex:\n",
    "            continue\n",
    "        ni = len(uniindex[i][0])\n",
    "        ri = 0\n",
    "        qfi = j\n",
    "        \n",
    "        for k,l in documentandlen.items():\n",
    "            if k in uniindex[i][0]:\n",
    "                fi = uniindex[i][0][k]\n",
    "            else:\n",
    "                fi = 0\n",
    "                \n",
    "            \n",
    "            K = k1 * ((1-b) + b * (l/avdl))\n",
    "            \n",
    "            a1 = 0.5 / (R - ri + 0.5)\n",
    "            a2 = (ni - ri + 0.5) / (N - ni - R + ri + 0.5)\n",
    "            a3 = (k1 + 1) * fi\n",
    "            a4 = K + fi\n",
    "            a5 = (k2 + 1) * qfi\n",
    "            a6 = k2 + qfi\n",
    "            ans = math.log(a1/a2)\n",
    "            ans = ans*a3*a5/(a4*a6)\n",
    "            \n",
    "            a[k] += ans\n",
    "        \n",
    "    \n",
    "    \n",
    "    return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makefiledict(location, typeoffile, typ = 2, filelist = [], stopwords = []):\n",
    "    diction = {}\n",
    "    if (typ == 2):\n",
    "        filelist = os.listdir(location)\n",
    "    for i in filelist:\n",
    "        if i.endswith(typeoffile):\n",
    "            with open(location + i, \"r\", encoding=\"utf-8\") as myfile:\n",
    "                contents=myfile.read().replace('\\n', ' ')\n",
    "                myfile.close()\n",
    "            contents = re.sub(pattern, \"\", contents)\n",
    "            tokens = word_tokenize(contents)\n",
    "            token=[token.lower() for token in tokens if (re.match(r'^[a-zA-Z0-9][ A-Za-z0-9-]*$', token))]\n",
    "            diction[i.split(typeoffile)[0]] = \"\"\n",
    "            newstring = []\n",
    "            for j in range(len(token)):\n",
    "                \n",
    "                string = token[j]\n",
    "                if string in stopwords:\n",
    "                    continue\n",
    "                if (string == \"html\" or string == \"pre\"):\n",
    "                    continue\n",
    "                if (string == \"pm\" or string == \"am\"):\n",
    "                    break\n",
    "                newstring.append(string)\n",
    "\n",
    "            diction[i.split(typeoffile)[0]] = \" \".join(newstring)\n",
    "    return diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchunigramdict(location, typeoffile, typ = 2, filelist = [], stopwords = []):\n",
    "\n",
    "    unigramdict = defaultdict(list)\n",
    "    documentandlen = {}\n",
    "    if (typ == 2):\n",
    "        filelist = os.listdir(location)\n",
    "    for i in filelist:\n",
    "        if i.endswith(typeoffile):\n",
    "            with open(location + i, \"r\", encoding=\"utf-8\") as myfile:\n",
    "                contents=myfile.read().replace('\\n', ' ')\n",
    "                myfile.close()\n",
    "            contents = re.sub(pattern, \"\", contents)\n",
    "            tokens = word_tokenize(contents)\n",
    "            token=[token.lower() for token in tokens if (re.match(r'^[a-zA-Z0-9][ A-Za-z0-9-]*$', token))]\n",
    "            documentandlen[i.split(typeoffile)[0]] = len(token)\n",
    "            countwordsdoc = 0\n",
    "            for j in range(len(token)):\n",
    "                \n",
    "                string = token[j]\n",
    "                if string in stopwords:\n",
    "                    continue\n",
    "                if (string == \"html\" or string == \"pre\"):\n",
    "                    continue\n",
    "                countwordsdoc += 1\n",
    "                if (string == \"pm\" or string == \"am\"):\n",
    "                    break\n",
    "                \n",
    "\n",
    "                if string in unigramdict:\n",
    "                    tempi = str(i.split(typeoffile)[0])\n",
    "                    if tempi in  unigramdict[string][0]:\n",
    "                        unigramdict[string][0][tempi] += 1\n",
    "                    else:\n",
    "                        unigramdict[string][0][tempi] = 1\n",
    "\n",
    "                else:\n",
    "                    tempi = str(i.split(typeoffile)[0])\n",
    "                    b = {}\n",
    "                    unigramdict[string].append(b)\n",
    "                    unigramdict[string][0][tempi] = 1\n",
    "            \n",
    "\n",
    "            documentandlen[i.split(typeoffile)[0]] = countwordsdoc\n",
    "    return unigramdict, documentandlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryfetcher(location):\n",
    "    unigramdict = defaultdict(list)\n",
    "    queries = []\n",
    "    a = \"\"\n",
    "    coudoc = 0\n",
    "\n",
    "    with open(location, \"r\", encoding=\"utf-8\") as myfile:\n",
    "        contents=myfile.read().replace('\\n', ' ')\n",
    "        myfile.close()\n",
    "    contents = re.sub(pattern, \"\", contents)\n",
    "    tokens = word_tokenize(contents)\n",
    "    token=[token.lower() for token in tokens if (re.match(r'^[a-zA-Z0-9][ A-Za-z0-9-]*$', token))]\n",
    "\n",
    "    for j in range(len(token)):\n",
    "\n",
    "        string = token[j]\n",
    "\n",
    "        if (string == \"doc\"):\n",
    "            continue\n",
    "        \n",
    "        if (string != \"docno\" and coudoc == 1):\n",
    "            continue\n",
    "        if (string == \"docno\"):\n",
    "            coudoc += 1\n",
    "        else:\n",
    "            if (a!=\"\"):\n",
    "                a += \" \" + string\n",
    "            else:\n",
    "                a += string\n",
    "\n",
    "        if (coudoc == 2):\n",
    "            if (a != \"\"):\n",
    "                queries.append(a)\n",
    "            coudoc = 0\n",
    "            a = \"\"\n",
    "    queries.append(a)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getstopwords(location):\n",
    "    with open(location, \"r\", encoding=\"utf-8\") as myfile:\n",
    "        contents=myfile.read().replace('\\n', ' ')\n",
    "        myfile.close()\n",
    "    stopwords = contents.split(\" \")\n",
    "    stopwords = list(filter(None, stopwords))\n",
    "    for i in range(len(stopwords)):\n",
    "        stopwords[i] = stopwords[i].lower()\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopquery(query, stopwords = []):\n",
    "    query = query.split(\" \")\n",
    "    newquery = []\n",
    "    for i in range(len(query)):\n",
    "        query[i] = query[i].lower()\n",
    "        if query[i] not in stopwords:\n",
    "            newquery.append(query[i])\n",
    "    return \" \".join(newquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readstemquery(location):\n",
    "    with open(location) as f:\n",
    "        stemquery = f.readlines()\n",
    "    stemquery = [x.strip() for x in stemquery]\n",
    "    stemquery = [x.lower() for x in stemquery]\n",
    "    return stemquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsnippet(query, diction, filename, k = 10):\n",
    "    comparestring = diction[filename]\n",
    "    comparestring = comparestring.split(\" \")\n",
    "    answerstring = \"\"\n",
    "    if k >= len(comparestring):\n",
    "        query = tokenizequery(query)\n",
    "        newstring = []\n",
    "        for j in comparestring:\n",
    "            if j in query:\n",
    "                    newstring.append(color.GO + j + color.END)\n",
    "            else:\n",
    "                newstring.append(j)\n",
    "        answerstring = \" \".join(newstring)\n",
    "    else:\n",
    "        query = tokenizequery(query)\n",
    "        maxcount = 0\n",
    "        for i in range(0, len(comparestring)-k+1):\n",
    "            maxc = 0\n",
    "            newstring = []\n",
    "            for j in range(i, i+k):\n",
    "                if comparestring[j] in query:\n",
    "                    maxc += 1\n",
    "                    newstring.append(color.GO + comparestring[j] + color.END)\n",
    "                else:\n",
    "                    newstring.append(comparestring[j])\n",
    "            if (maxc > maxcount):\n",
    "                maxcount = maxc\n",
    "                answerstring = \" \".join(newstring)\n",
    "    return answerstring    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizequery(query):\n",
    "    tokens = word_tokenize(query)\n",
    "    token=[token.lower() for token in tokens if (re.match(r'^[a-zA-Z0-9][ A-Za-z0-9-]*$', token))]\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchrel(location):\n",
    "    with open(location) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    reldict = defaultdict(list)\n",
    "    for i in content:\n",
    "        i = i.split(\" \")\n",
    "        reldict[i[0]].append(i[2])\n",
    "    return reldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createstemdictionary(listwords):\n",
    "    stemdiction = defaultdict(list)\n",
    "    ps = PorterStemmer()\n",
    "    for i in listwords:\n",
    "        if ps.stem(i) not in stemdiction:\n",
    "            stemdiction[ps.stem(i)].append(i)\n",
    "        else:\n",
    "            stemdiction[ps.stem(i)].append(i)\n",
    "    return stemdiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getstemqueries(query, stemdiction, n = 10):\n",
    "    query = tokenizequery(query)\n",
    "    ps = PorterStemmer()\n",
    "    c = 0\n",
    "    for i in query:\n",
    "        if ps.stem(i) in stemdiction:\n",
    "            for j in stemdiction[ps.stem(i)]:\n",
    "                if j not in query:\n",
    "                    query.append(j)\n",
    "                    c += 1\n",
    "                    break\n",
    "       \n",
    "        if (c == n):\n",
    "            break\n",
    "    return \" \".join(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordslist = getstopwords(location = \"corpus/common_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacmq = queryfetcher(\"cacmquery/cacm.query.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacmunigramdict, cacmlen = fetchunigramdict(location = \"cacm/\", typeoffile = \".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reldict = fetchrel(\"corpus/cacm.rel.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalue = pd.DataFrame(columns=['Query', 'Avg Precision', 'Rank Reciprocal', 'Precision at 5', 'Precision at 20'])\n",
    "precrecdict = {}\n",
    "for i in range(len(cacmq)):\n",
    "    an1 = BM25(cacmq[i], documentandlen = cacmlen, uniindex = cacmunigramdict)\n",
    "    an11 = sorted(an1.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    an11 = pd.DataFrame(an11)\n",
    "    s = \"Query_\" + str(i+1)\n",
    "    f= open(\"Phase 1/Task_1/BM25/Query Results/\" + s + \".txt\",\"w+\",encoding=\"utf-8\")\n",
    "    f.write(\"QUERY = \\\"\" + cacmq[i] + \"\\\"\\n\")\n",
    "    for j in range(100):\n",
    "        k = i+1\n",
    "        f.write(str((str(k) + \" Q0 \" + str(an11[0][j]) +\" \"+ str(j+1) +\" \" + str(an11[1][j]) + \" BM25\" + \"\\n\")))\n",
    "    f.close()\n",
    "    if str(i+1) not in reldict:\n",
    "        continue        \n",
    "    evalue.loc[-1] = [cacmq[i],\n",
    "                      getavgprecision(an11, reldict, i+1) ,\n",
    "                      reciprocalrank(an11, reldict, i+1),\n",
    "                      precatk(an11, reldict, i+1, 5),\n",
    "                      precatk(an11, reldict, i+1, 20)]\n",
    "    evalue.index = evalue.index + 1\n",
    "    precrecdict[str(i+1)] = get_full_precision_recall_table(an11, reldict, i+1)\n",
    "    precrecdict[str(i+1)].to_csv(r'Phase 1/Task_1/BM25/Precision and Recall/Query_' + str(i+1) + '.csv',index=False)\n",
    "evalue = evalue.reset_index(drop = True)\n",
    "f= open(\"Phase 1/Task_1/BM25/MAP.txt\",\"w+\")\n",
    "f.write(\"MAP = \")\n",
    "f.write(str(evalue['Avg Precision'].mean()))\n",
    "f.close() \n",
    "f= open(\"Phase 1/Task_1/BM25/MRR.txt\",\"w+\")\n",
    "f.write(\"MRR = \")\n",
    "f.write(str(evalue['Rank Reciprocal'].mean()))\n",
    "f.close() \n",
    "evalue[['Query','Precision at 5', 'Precision at 20']].to_csv(r'Phase 1/Task_1/BM25/P@K.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalue = pd.DataFrame(columns=['Query', 'Avg Precision', 'Rank Reciprocal', 'Precision at 5', 'Precision at 20'])\n",
    "precrecdict = {}\n",
    "for i in range(len(cacmq)):\n",
    "    an1 = tfidf(cacmq[i], documentandlen = cacmlen, uniindex = cacmunigramdict)\n",
    "    an11 = sorted(an1.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    an11 = pd.DataFrame(an11)\n",
    "    s = \"Query_\" + str(i+1)\n",
    "    f= open(\"Phase 1/Task_1/TF-IDF/Query Results/\" + s + \".txt\",\"w+\",encoding=\"utf-8\")\n",
    "    f.write(\"QUERY = \\\"\" + cacmq[i] + \"\\\"\\n\")\n",
    "    for j in range(100):\n",
    "        k = i+1\n",
    "        f.write(str((str(k) + \" Q0 \" + str(an11[0][j]) +\" \"+ str(j+1) +\" \" + str(an11[1][j]) + \" tf-idf\" + \"\\n\")))\n",
    "    f.close()\n",
    "    if str(i+1) not in reldict:\n",
    "        continue        \n",
    "    evalue.loc[-1] = [cacmq[i],\n",
    "                      getavgprecision(an11, reldict, i+1) ,\n",
    "                      reciprocalrank(an11, reldict, i+1),\n",
    "                      precatk(an11, reldict, i+1, 5),\n",
    "                      precatk(an11, reldict, i+1, 20)]\n",
    "    evalue.index = evalue.index + 1\n",
    "    precrecdict[str(i+1)] = get_full_precision_recall_table(an11, reldict, i+1)\n",
    "    precrecdict[str(i+1)].to_csv(r'Phase 1/Task_1/TF-IDF/Precision and Recall/Query_' + str(i+1) + '.csv',index=False)\n",
    "evalue = evalue.reset_index(drop = True)\n",
    "f= open(\"Phase 1/Task_1/TF-IDF/MAP.txt\",\"w+\")\n",
    "f.write(\"MAP = \")\n",
    "f.write(str(evalue['Avg Precision'].mean()))\n",
    "f.close() \n",
    "f= open(\"Phase 1/Task_1/TF-IDF/MRR.txt\",\"w+\")\n",
    "f.write(\"MRR = \")\n",
    "f.write(str(evalue['Rank Reciprocal'].mean()))\n",
    "f.close() \n",
    "evalue[['Query','Precision at 5', 'Precision at 20']].to_csv(r'Phase 1/Task_1/TF-IDF/P@K.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Likelihood Model [Dirichlet Smoothing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalue = pd.DataFrame(columns=['Query', 'Avg Precision', 'Rank Reciprocal', 'Precision at 5', 'Precision at 20'])\n",
    "precrecdict = {}\n",
    "for i in range(len(cacmq)):\n",
    "    an1 = querylikeDirichlet(cacmq[i], documentandlen = cacmlen, uniindex = cacmunigramdict)\n",
    "    an11 = sorted(an1.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    an11 = pd.DataFrame(an11)\n",
    "    s = \"Query_\" + str(i+1)\n",
    "    f= open(\"Phase 1/Task_1/QLMDirichlet/Query Results/\" + s + \".txt\",\"w+\",encoding=\"utf-8\")\n",
    "    f.write(\"QUERY = \\\"\" + cacmq[i] + \"\\\"\\n\")\n",
    "    for j in range(100):\n",
    "        k = i+1\n",
    "        f.write(str((str(k) + \" Q0 \" + str(an11[0][j]) +\" \"+ str(j+1) +\" \" + str(an11[1][j]) + \" QueryLikelihoodDirichlet\" + \"\\n\")))\n",
    "    f.close()\n",
    "    if str(i+1) not in reldict:\n",
    "        continue        \n",
    "    evalue.loc[-1] = [cacmq[i],\n",
    "                      getavgprecision(an11, reldict, i+1) ,\n",
    "                      reciprocalrank(an11, reldict, i+1),\n",
    "                      precatk(an11, reldict, i+1, 5),\n",
    "                      precatk(an11, reldict, i+1, 20)]\n",
    "    evalue.index = evalue.index + 1\n",
    "    precrecdict[str(i+1)] = get_full_precision_recall_table(an11, reldict, i+1)\n",
    "    precrecdict[str(i+1)].to_csv(r'Phase 1/Task_1/QLMDirichlet/Precision and Recall/Query_' + str(i+1) + '.csv',index=False)\n",
    "evalue = evalue.reset_index(drop = True)\n",
    "f= open(\"Phase 1/Task_1/QLMDirichlet/MAP.txt\",\"w+\")\n",
    "f.write(\"MAP = \")\n",
    "f.write(str(evalue['Avg Precision'].mean()))\n",
    "f.close() \n",
    "f= open(\"Phase 1/Task_1/QLMDirichlet/MRR.txt\",\"w+\")\n",
    "f.write(\"MRR = \")\n",
    "f.write(str(evalue['Rank Reciprocal'].mean()))\n",
    "f.close() \n",
    "evalue[['Query','Precision at 5', 'Precision at 20']].to_csv(r'Phase 1/Task_1/QLMDirichlet/P@K.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Time Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computerization', 'computerized', 'computerize']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = createstemdictionary(cacmunigramdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalue = pd.DataFrame(columns=['Query', 'Avg Precision', 'Rank Reciprocal', 'Precision at 5', 'Precision at 20'])\n",
    "precrecdict = {}\n",
    "stemqueries = []\n",
    "for i in range(len(cacmq)):\n",
    "    stemqueries.append(getstemqueries(cacmq[i], z))\n",
    "    an1 = BM25(stemqueries[i], documentandlen = cacmlen, uniindex = cacmunigramdict)\n",
    "    an11 = sorted(an1.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    an11 = pd.DataFrame(an11)\n",
    "    s = \"Query_\" + str(i+1)\n",
    "    f= open(\"Phase 1/Task_2/QueryStemming/Query Results/\" + s + \".txt\",\"w+\",encoding=\"utf-8\")\n",
    "    f.write(\"QUERY = \\\"\" + stemqueries[i] + \"\\\"\\n\")\n",
    "    for j in range(100):\n",
    "        k = i+1\n",
    "        f.write(str((str(k) + \" Q0 \" + str(an11[0][j]) +\" \"+ str(j+1) +\" \" + str(an11[1][j]) + \" BM25\" + \"\\n\")))\n",
    "    f.close()\n",
    "    if str(i+1) not in reldict:\n",
    "        continue        \n",
    "    evalue.loc[-1] = [stemqueries[i],\n",
    "                      getavgprecision(an11, reldict, i+1) ,\n",
    "                      reciprocalrank(an11, reldict, i+1),\n",
    "                      precatk(an11, reldict, i+1, 5),\n",
    "                      precatk(an11, reldict, i+1, 20)]\n",
    "    evalue.index = evalue.index + 1\n",
    "    precrecdict[str(i+1)] = get_full_precision_recall_table(an11, reldict, i+1)\n",
    "    precrecdict[str(i+1)].to_csv(r'Phase 1/Task_2/QueryStemming/Precision and Recall/Query_' + str(i+1) + '.csv',index=False)\n",
    "evalue = evalue.reset_index(drop = True)\n",
    "f= open(\"Phase 1/Task_2/QueryStemming/MAP.txt\",\"w+\")\n",
    "f.write(\"MAP = \")\n",
    "f.write(str(evalue['Avg Precision'].mean()))\n",
    "f.close() \n",
    "f= open(\"Phase 1/Task_2/QueryStemming/MRR.txt\",\"w+\")\n",
    "f.write(\"MRR = \")\n",
    "f.write(str(evalue['Rank Reciprocal'].mean()))\n",
    "f.close() \n",
    "evalue[['Query','Precision at 5', 'Precision at 20']].to_csv(r'Phase 1/Task_2/QueryStemming/P@K.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Psuedo Relevance Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalue = pd.DataFrame(columns=['Query', 'Avg Precision', 'Rank Reciprocal', 'Precision at 5', 'Precision at 20'])\n",
    "precrecdict = {}\n",
    "newquery = []\n",
    "for i in range(len(cacmq)):\n",
    "    newquery.append(psuedorel(query = cacmq[i], location = \"cacm/\", typeoffile=\".html\", stopwords = stopwordslist))\n",
    "    an1 = BM25(newquery[i], documentandlen = cacmlen, uniindex = cacmunigramdict)\n",
    "    an11 = sorted(an1.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    an11 = pd.DataFrame(an11)\n",
    "    s = \"Query_\" + str(i+1)\n",
    "    f= open(\"Phase 1/Task_2/PsuedoRelevance/Query Results/\" + s + \".txt\",\"w+\",encoding=\"utf-8\")\n",
    "    f.write(\"QUERY = \\\"\" + newquery[i] + \"\\\"\\n\")\n",
    "    for j in range(100):\n",
    "        k = i+1\n",
    "        f.write(str((str(k) + \" Q0 \" + str(an11[0][j]) +\" \"+ str(j+1) +\" \" + str(an11[1][j]) + \" BM25\" + \"\\n\")))\n",
    "    f.close()\n",
    "    if str(i+1) not in reldict:\n",
    "        continue        \n",
    "    evalue.loc[-1] = [newquery[i],\n",
    "                      getavgprecision(an11, reldict, i+1) ,\n",
    "                      reciprocalrank(an11, reldict, i+1),\n",
    "                      precatk(an11, reldict, i+1, 5),\n",
    "                      precatk(an11, reldict, i+1, 20)]\n",
    "    evalue.index = evalue.index + 1\n",
    "    precrecdict[str(i+1)] = get_full_precision_recall_table(an11, reldict, i+1)\n",
    "    precrecdict[str(i+1)].to_csv(r'Phase 1/Task_2/PsuedoRelevance/Precision and Recall/Query_' + str(i+1) + '.csv',index=False)\n",
    "evalue = evalue.reset_index(drop = True)\n",
    "f= open(\"Phase 1/Task_2/PsuedoRelevance/MAP.txt\",\"w+\")\n",
    "f.write(\"MAP = \")\n",
    "f.write(str(evalue['Avg Precision'].mean()))\n",
    "f.close() \n",
    "f= open(\"Phase 1/Task_2/PsuedoRelevance/MRR.txt\",\"w+\")\n",
    "f.write(\"MRR = \")\n",
    "f.write(str(evalue['Rank Reciprocal'].mean()))\n",
    "f.close() \n",
    "evalue[['Query','Precision at 5', 'Precision at 20']].to_csv(r'Phase 1/Task_2/PsuedoRelevance/P@K.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordslist = getstopwords(location = \"corpus/common_words.txt\")\n",
    "cacmq = queryfetcher(\"cacmquery/cacm.query.txt\")\n",
    "stopdict, stoplen = fetchunigramdict(location = \"cacm/\", typeoffile = \".html\", stopwords = stopwordslist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping - BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalue = pd.DataFrame(columns=['Query', 'Avg Precision', 'Rank Reciprocal', 'Precision at 5', 'Precision at 20'])\n",
    "precrecdict = {}\n",
    "stoppedqueries = []\n",
    "for i in range(len(cacmq)):\n",
    "    stoppedqueries.append(stopquery(cacmq[i], stopwords = stopwordslist))\n",
    "    an1 = BM25(stoppedqueries[i], documentandlen = stoplen, uniindex = stopdict)\n",
    "    an11 = sorted(an1.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    an11 = pd.DataFrame(an11)\n",
    "    s = \"Query_\" + str(i+1)\n",
    "    f= open(\"Phase 1/Task_3/Stopping/BM25/Query Results/\" + s + \".txt\",\"w+\",encoding=\"utf-8\")\n",
    "    f.write(\"QUERY = \\\"\" + stoppedqueries[i] + \"\\\"\\n\")\n",
    "    for j in range(100):\n",
    "        k = i+1\n",
    "        f.write(str((str(k) + \" Q0 \" + str(an11[0][j]) +\" \"+ str(j+1) +\" \" + str(an11[1][j]) + \" BM25\" + \"\\n\")))\n",
    "    f.close()\n",
    "    if str(i+1) not in reldict:\n",
    "        continue        \n",
    "    evalue.loc[-1] = [stoppedqueries[i],\n",
    "                      getavgprecision(an11, reldict, i+1) ,\n",
    "                      reciprocalrank(an11, reldict, i+1),\n",
    "                      precatk(an11, reldict, i+1, 5),\n",
    "                      precatk(an11, reldict, i+1, 20)]\n",
    "    evalue.index = evalue.index + 1\n",
    "    precrecdict[str(i+1)] = get_full_precision_recall_table(an11, reldict, i+1)\n",
    "    precrecdict[str(i+1)].to_csv(r'Phase 1/Task_3/Stopping/BM25/Precision and Recall/Query_' + str(i+1) + '.csv',index=False)\n",
    "evalue = evalue.reset_index(drop = True)\n",
    "f= open(\"Phase 1/Task_3/Stopping/BM25/MAP.txt\",\"w+\")\n",
    "f.write(\"MAP = \")\n",
    "f.write(str(evalue['Avg Precision'].mean()))\n",
    "f.close() \n",
    "f= open(\"Phase 1/Task_3/Stopping/BM25/MRR.txt\",\"w+\")\n",
    "f.write(\"MRR = \")\n",
    "f.write(str(evalue['Rank Reciprocal'].mean()))\n",
    "f.close() \n",
    "evalue[['Query','Precision at 5', 'Precision at 20']].to_csv(r'Phase 1/Task_3/Stopping/BM25/P@K.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalue = pd.DataFrame(columns=['Query', 'Avg Precision', 'Rank Reciprocal', 'Precision at 5', 'Precision at 20'])\n",
    "precrecdict = {}\n",
    "stoppedqueries = []\n",
    "for i in range(len(cacmq)):\n",
    "    stoppedqueries.append(stopquery(cacmq[i], stopwords = stopwordslist))\n",
    "    an1 = tfidf(stoppedqueries[i], documentandlen = stoplen, uniindex = stopdict)\n",
    "    an11 = sorted(an1.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    an11 = pd.DataFrame(an11)\n",
    "    s = \"Query_\" + str(i+1)\n",
    "    f= open(\"Phase 1/Task_3/Stopping/TF-IDF/Query Results/\" + s + \".txt\",\"w+\",encoding=\"utf-8\")\n",
    "    f.write(\"QUERY = \\\"\" + stoppedqueries[i] + \"\\\"\\n\")\n",
    "    for j in range(100):\n",
    "        k = i+1\n",
    "        f.write(str((str(k) + \" Q0 \" + str(an11[0][j]) +\" \"+ str(j+1) +\" \" + str(an11[1][j]) + \" tf-idf\" + \"\\n\")))\n",
    "    f.close()\n",
    "    if str(i+1) not in reldict:\n",
    "        continue        \n",
    "    evalue.loc[-1] = [stoppedqueries[i],\n",
    "                      getavgprecision(an11, reldict, i+1) ,\n",
    "                      reciprocalrank(an11, reldict, i+1),\n",
    "                      precatk(an11, reldict, i+1, 5),\n",
    "                      precatk(an11, reldict, i+1, 20)]\n",
    "    evalue.index = evalue.index + 1\n",
    "    precrecdict[str(i+1)] = get_full_precision_recall_table(an11, reldict, i+1)\n",
    "    precrecdict[str(i+1)].to_csv(r'Phase 1/Task_3/Stopping/TF-IDF/Precision and Recall/Query_' + str(i+1) + '.csv',index=False)\n",
    "evalue = evalue.reset_index(drop = True)\n",
    "f= open(\"Phase 1/Task_3/Stopping/TF-IDF/MAP.txt\",\"w+\")\n",
    "f.write(\"MAP = \")\n",
    "f.write(str(evalue['Avg Precision'].mean()))\n",
    "f.close() \n",
    "f= open(\"Phase 1/Task_3/Stopping/TF-IDF/MRR.txt\",\"w+\")\n",
    "f.write(\"MRR = \")\n",
    "f.write(str(evalue['Rank Reciprocal'].mean()))\n",
    "f.close() \n",
    "evalue[['Query','Precision at 5', 'Precision at 20']].to_csv(r'Phase 1/Task_3/Stopping/TF-IDF/P@K.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMED CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemdict, stemlen = readstemcorpus(location = \"corpus/cacm_stem.txt\", typeoffile = \".txt\")\n",
    "stemquery = readstemquery(location = \"corpus/cacm_stem.query.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalue = pd.DataFrame(columns=['Query', 'Avg Precision', 'Rank Reciprocal', 'Precision at 5', 'Precision at 20'])\n",
    "precrecdict = {}\n",
    "for i in range(len(stemquery)):\n",
    "    an1 = BM25(stemquery[i], documentandlen = stemlen, uniindex = stemdict)\n",
    "    an11 = sorted(an1.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    an11 = pd.DataFrame(an11)\n",
    "    s = \"Query_\" + str(i+1)\n",
    "    f= open(\"Phase 1/Task_3/Stemmed/BM25/Query Results/\" + s + \".txt\",\"w+\",encoding=\"utf-8\")\n",
    "    f.write(\"QUERY = \\\"\" + stemquery[i] + \"\\\"\\n\")\n",
    "    for j in range(100):\n",
    "        k = i+1\n",
    "        f.write(str((str(k) + \" Q0 \" + str(an11[0][j]) +\" \"+ str(j+1) +\" \" + str(an11[1][j]) + \" BM25\" + \"\\n\")))\n",
    "    f.close()\n",
    "    if str(i+1) not in reldict:\n",
    "        continue        \n",
    "    evalue.loc[-1] = [stemquery[i],\n",
    "                      getavgprecision(an11, reldict, i+1) ,\n",
    "                      reciprocalrank(an11, reldict, i+1),\n",
    "                      precatk(an11, reldict, i+1, 5),\n",
    "                      precatk(an11, reldict, i+1, 20)]\n",
    "    evalue.index = evalue.index + 1\n",
    "    precrecdict[str(i+1)] = get_full_precision_recall_table(an11, reldict, i+1)\n",
    "    precrecdict[str(i+1)].to_csv(r'Phase 1/Task_3/Stemmed/BM25/Precision and Recall/Query_' + str(i+1) + '.csv',index=False)\n",
    "evalue = evalue.reset_index(drop = True)\n",
    "f= open(\"Phase 1/Task_3/Stemmed/BM25/MAP.txt\",\"w+\")\n",
    "f.write(\"MAP = \")\n",
    "f.write(str(evalue['Avg Precision'].mean()))\n",
    "f.close() \n",
    "f= open(\"Phase 1/Task_3/Stemmed/BM25/MRR.txt\",\"w+\")\n",
    "f.write(\"MRR = \")\n",
    "f.write(str(evalue['Rank Reciprocal'].mean()))\n",
    "f.close() \n",
    "evalue[['Query','Precision at 5', 'Precision at 20']].to_csv(r'Phase 1/Task_3/Stemmed/BM25/P@K.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalue = pd.DataFrame(columns=['Query', 'Avg Precision', 'Rank Reciprocal', 'Precision at 5', 'Precision at 20'])\n",
    "precrecdict = {}\n",
    "for i in range(len(stemquery)):\n",
    "    an1 = tfidf(stemquery[i], documentandlen = stemlen, uniindex = stemdict)\n",
    "    an11 = sorted(an1.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    an11 = pd.DataFrame(an11)\n",
    "    s = \"Query_\" + str(i+1)\n",
    "    f= open(\"Phase 1/Task_3/Stemmed/TF-IDF/Query Results/\" + s + \".txt\",\"w+\",encoding=\"utf-8\")\n",
    "    f.write(\"QUERY = \\\"\" + stemquery[i] + \"\\\"\\n\")\n",
    "    for j in range(100):\n",
    "        k = i+1\n",
    "        f.write(str((str(k) + \" Q0 \" + str(an11[0][j]) +\" \"+ str(j+1) +\" \" + str(an11[1][j]) + \" tf-idf\" + \"\\n\")))\n",
    "    f.close()\n",
    "    if str(i+1) not in reldict:\n",
    "        continue        \n",
    "    evalue.loc[-1] = [stemquery[i],\n",
    "                      getavgprecision(an11, reldict, i+1) ,\n",
    "                      reciprocalrank(an11, reldict, i+1),\n",
    "                      precatk(an11, reldict, i+1, 5),\n",
    "                      precatk(an11, reldict, i+1, 20)]\n",
    "    evalue.index = evalue.index + 1\n",
    "    precrecdict[str(i+1)] = get_full_precision_recall_table(an11, reldict, i+1)\n",
    "    precrecdict[str(i+1)].to_csv(r'Phase 1/Task_3/Stemmed/TF-IDF/Precision and Recall/Query_' + str(i+1) + '.csv',index=False)\n",
    "evalue = evalue.reset_index(drop = True)\n",
    "f= open(\"Phase 1/Task_3/Stemmed/TF-IDF/MAP.txt\",\"w+\")\n",
    "f.write(\"MAP = \")\n",
    "f.write(str(evalue['Avg Precision'].mean()))\n",
    "f.close() \n",
    "f= open(\"Phase 1/Task_3/Stemmed/TF-IDF/MRR.txt\",\"w+\")\n",
    "f.write(\"MRR = \")\n",
    "f.write(str(evalue['Rank Reciprocal'].mean()))\n",
    "f.close() \n",
    "evalue[['Query','Precision at 5', 'Precision at 20']].to_csv(r'Phase 1/Task_3/Stemmed/TF-IDF/P@K.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE - 2 [Snippet Generation and Highlighting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "diction2 = makefiledict(location = \"cacm/\", typeoffile = \".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Query : algorithms rule\n",
      "\n",
      "\n",
      "Result: 1\n",
      "Document Name : \u001b[1mCACM-2273\u001b[0m\n",
      "tables by \u001b[0;30;43mrule\u001b[0m mask method without \u001b[0;30;43mrule\u001b[0m mask two \u001b[0;30;43malgorithms\u001b[0m\n",
      "\n",
      "\n",
      "Result: 2\n",
      "Document Name : \u001b[1mCACM-2239\u001b[0m\n",
      "numerical integration integration \u001b[0;30;43mrule\u001b[0m adaptive integration automatic integration simpsons \u001b[0;30;43mrule\u001b[0m\n",
      "\n",
      "\n",
      "Result: 3\n",
      "Document Name : \u001b[1mCACM-2074\u001b[0m\n",
      "numerical integration integration \u001b[0;30;43mrule\u001b[0m adaptive integration automatic integration simpsons \u001b[0;30;43mrule\u001b[0m\n",
      "\n",
      "\n",
      "Result: 4\n",
      "Document Name : \u001b[1mCACM-2263\u001b[0m\n",
      "optimal and near-optimal flowcharts two new \u001b[0;30;43malgorithms\u001b[0m two new \u001b[0;30;43malgorithms\u001b[0m\n",
      "\n",
      "\n",
      "Result: 5\n",
      "Document Name : \u001b[1mCACM-2009\u001b[0m\n",
      "simpsons \u001b[0;30;43mrule\u001b[0m for multiple integration algorithm 233 d1 cacm august\n",
      "\n",
      "\n",
      "Result: 6\n",
      "Document Name : \u001b[1mCACM-1672\u001b[0m\n",
      "the integration of periodic analytic functions by the trapezoidal \u001b[0;30;43mrule\u001b[0m\n",
      "\n",
      "\n",
      "Result: 7\n",
      "Document Name : \u001b[1mCACM-1909\u001b[0m\n",
      "numerical integration of its integral representation using the trapezoidal \u001b[0;30;43mrule\u001b[0m\n",
      "\n",
      "\n",
      "Result: 8\n",
      "Document Name : \u001b[1mCACM-0570\u001b[0m\n",
      "simpsons \u001b[0;30;43mrule\u001b[0m integrator algorithm 103 cacm june 1962 kuncir g\n",
      "\n",
      "\n",
      "Result: 9\n",
      "Document Name : \u001b[1mCACM-1058\u001b[0m\n",
      "simpsons \u001b[0;30;43mrule\u001b[0m for multiple integration algorithm 233 cacm june 1964\n",
      "\n",
      "\n",
      "Result: 10\n",
      "Document Name : \u001b[1mCACM-0429\u001b[0m\n",
      "adaptive nimerical integration by simpsons \u001b[0;30;43mrule\u001b[0m algorithm 145 cacm december\n"
     ]
    }
   ],
   "source": [
    "p2query = input(\"Enter Your Query : \")\n",
    "phase2 = BM25(p2query, documentandlen = cacmlen, uniindex = cacmunigramdict)\n",
    "phase21 = sorted(phase2.items(), key=operator.itemgetter(1), reverse = True)\n",
    "phase21 = pd.DataFrame(phase21)\n",
    "for i in range(10):\n",
    "    print(\"\\n\")\n",
    "    print(\"Result:\",i+1)\n",
    "    print(\"Document Name :\", color.BOLD + phase21[0][i] + color.END)\n",
    "    print(getsnippet(p2query, diction2, phase21[0][i], k = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOPPING WITH EXPANSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordslist = getstopwords(location = \"corpus/common_words.txt\")\n",
    "cacmq = queryfetcher(\"cacmquery/cacm.query.txt\")\n",
    "stopdict, stoplen = fetchunigramdict(location = \"cacm/\", typeoffile = \".html\", stopwords = stopwordslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalue = pd.DataFrame(columns=['Query', 'Avg Precision', 'Rank Reciprocal', 'Precision at 5', 'Precision at 20'])\n",
    "precrecdict = {}\n",
    "newquery = []\n",
    "stoppedqueries = []\n",
    "for i in range(len(cacmq)):\n",
    "    stoppedqueries.append(stopquery(cacmq[i], stopwords = stopwordslist))\n",
    "    newquery.append(psuedorel(query = stoppedqueries[i], location = \"cacm/\", typeoffile=\".html\", stopwords = stopwordslist))\n",
    "    an1 = BM25(newquery[i], documentandlen = stoplen, uniindex = stopdict)\n",
    "    an11 = sorted(an1.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    an11 = pd.DataFrame(an11)\n",
    "    s = \"Query_\" + str(i+1)\n",
    "    f= open(\"Phase 3/StoppingExpansion/Query Results/\" + s + \".txt\",\"w+\",encoding=\"utf-8\")\n",
    "    f.write(\"QUERY = \\\"\" + newquery[i] + \"\\\"\\n\")\n",
    "    for j in range(100):\n",
    "        k = i+1\n",
    "        f.write(str((str(k) + \" Q0 \" + str(an11[0][j]) +\" \"+ str(j+1) +\" \" + str(an11[1][j]) + \" BM25\" + \"\\n\")))\n",
    "    f.close()\n",
    "    if str(i+1) not in reldict:\n",
    "        continue        \n",
    "    evalue.loc[-1] = [newquery[i],\n",
    "                      getavgprecision(an11, reldict, i+1) ,\n",
    "                      reciprocalrank(an11, reldict, i+1),\n",
    "                      precatk(an11, reldict, i+1, 5),\n",
    "                      precatk(an11, reldict, i+1, 20)]\n",
    "    evalue.index = evalue.index + 1\n",
    "    precrecdict[str(i+1)] = get_full_precision_recall_table(an11, reldict, i+1)\n",
    "    precrecdict[str(i+1)].to_csv(r'Phase 3/StoppingExpansion/Precision and Recall/Query_' + str(i+1) + '.csv',index=False)\n",
    "evalue = evalue.reset_index(drop = True)\n",
    "f= open(\"Phase 3/StoppingExpansion/MAP.txt\",\"w+\")\n",
    "f.write(\"MAP = \")\n",
    "f.write(str(evalue['Avg Precision'].mean()))\n",
    "f.close() \n",
    "f= open(\"Phase 3/StoppingExpansion/MRR.txt\",\"w+\")\n",
    "f.write(\"MRR = \")\n",
    "f.write(str(evalue['Rank Reciprocal'].mean()))\n",
    "f.close() \n",
    "evalue[['Query','Precision at 5', 'Precision at 20']].to_csv(r'Phase 3/StoppingExpansion/P@K.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocalrank(results, reldict, queryid):\n",
    "    for i in range(len(results)):\n",
    "        if results[0][i] in reldict[str(queryid)]:\n",
    "            return 1/(i+1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getavgprecision(results, reldict, queryid):\n",
    "    rel = 0\n",
    "    ans = 0\n",
    "    for i in range(len(results)):\n",
    "        if results[0][i] in reldict[str(queryid)]:\n",
    "            rel +=1\n",
    "            ans += rel/(i+1)\n",
    "    if (rel == 0):\n",
    "        return 0\n",
    "    return ans/rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precatk(results, reldict, queryid, k):\n",
    "    rel = 0\n",
    "    for i in range(k):\n",
    "        if results[0][i] in reldict[str(queryid)]:\n",
    "            rel +=1    \n",
    "    return rel/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_precision_recall_table(results, reldict, queryid):\n",
    "    evalue = pd.DataFrame(columns=[\"Result\",\"Precision\", \"Recall\"])\n",
    "    rel = 0\n",
    "    reldoc = len(reldict[str(queryid)])\n",
    "    if (reldoc == 0):\n",
    "        print(str(queryid))\n",
    "    for i in range(100):\n",
    "        if results[0][i] in reldict[str(queryid)]:\n",
    "            rel +=1 \n",
    "        evalue.loc[-1] = [results[0][i],rel/(i+1),rel/reldoc]\n",
    "        evalue.index = evalue.index + 1\n",
    "    return evalue.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA-CREDIT [Spelling Correction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "diction1 = makefiledict(location = \"cacm/\", typeoffile = \".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Query : algorthm computre is good tiem\n",
      "Did you Mean :  \u001b[1malgorithm\u001b[0m \u001b[1mcompute\u001b[0m is good \u001b[1mthem\u001b[0m\n",
      "\n",
      "\n",
      "Did you Mean :  \u001b[1malgorithms\u001b[0m \u001b[1mcomputers\u001b[0m is good \u001b[1mties\u001b[0m\n",
      "\n",
      "\n",
      "Did you Mean :  \u001b[1malgorithmic\u001b[0m \u001b[1mcomputer\u001b[0m is good \u001b[1mtied\u001b[0m\n",
      "\n",
      "\n",
      "Did you Mean :  \u001b[1malthm\u001b[0m \u001b[1mcompare\u001b[0m is good \u001b[1mthe\u001b[0m\n",
      "\n",
      "\n",
      "Did you Mean :  \u001b[1malgol\u001b[0m \u001b[1mcomputed\u001b[0m is good \u001b[1mibm\u001b[0m\n",
      "\n",
      "\n",
      "Did you Mean :  \u001b[1maegerter\u001b[0m \u001b[1mcomputes\u001b[0m is good \u001b[1mtime\u001b[0m\n",
      "\n",
      "\n",
      "Enter your Query : algorithm compute is good them\n",
      "\n",
      "\n",
      "Result: 1\n",
      "Document Name : \u001b[1mCACM-2460\u001b[0m\n",
      "clenshaw-curtis quadrature \u001b[0;30;43malgorithm\u001b[0m r424 cacm august 1973 \u001b[0;30;43mgood\u001b[0m a j\n",
      "\n",
      "\n",
      "Result: 2\n",
      "Document Name : \u001b[1mCACM-2986\u001b[0m\n",
      "grammar can be done by a practical \u001b[0;30;43malgorithm\u001b[0m that \u001b[0;30;43mis\u001b[0m\n",
      "\n",
      "\n",
      "Result: 3\n",
      "Document Name : \u001b[1mCACM-1525\u001b[0m\n",
      "a power of two computing time for this \u001b[0;30;43malgorithm\u001b[0m \u001b[0;30;43mis\u001b[0m\n",
      "\n",
      "\n",
      "Result: 4\n",
      "Document Name : \u001b[1mCACM-3069\u001b[0m\n",
      "analysis \u001b[0;30;43malgorithm\u001b[0m a new interprocedural data flow analysis \u001b[0;30;43malgorithm\u001b[0m \u001b[0;30;43mis\u001b[0m\n",
      "\n",
      "\n",
      "Result: 5\n",
      "Document Name : \u001b[1mCACM-2520\u001b[0m\n",
      "multipliers \u001b[0;30;43malgorithm\u001b[0m c386 cacm april 1973 ragland l c \u001b[0;30;43mgood\u001b[0m\n",
      "\n",
      "\n",
      "Result: 6\n",
      "Document Name : \u001b[1mCACM-2547\u001b[0m\n",
      "pictorial features contour maps region coverage and line structures \u001b[0;30;43mis\u001b[0m\n",
      "\n",
      "\n",
      "Result: 7\n",
      "Document Name : \u001b[1mCACM-0222\u001b[0m\n",
      "coding of external symbols into symbols internal to a \u001b[0;30;43mcompute\u001b[0m\n",
      "\n",
      "\n",
      "Result: 8\n",
      "Document Name : \u001b[1mCACM-1206\u001b[0m\n",
      "sjp \u001b[0;30;43mis\u001b[0m used to discard wrong information and to \u001b[0;30;43mcompute\u001b[0m\n",
      "\n",
      "\n",
      "Result: 9\n",
      "Document Name : \u001b[1mCACM-1909\u001b[0m\n",
      "numerical integration it \u001b[0;30;43mis\u001b[0m shown to be practical to \u001b[0;30;43mcompute\u001b[0m\n",
      "\n",
      "\n",
      "Result: 10\n",
      "Document Name : \u001b[1mCACM-1619\u001b[0m\n",
      "discussed computational error generated by some algorithms used to \u001b[0;30;43mcompute\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "querycheck = 0\n",
    "while (querycheck == 0):\n",
    "    quer = input(\"Enter your Query : \")\n",
    "    quer1 = tokenizequery(quer)\n",
    "    diction = cacmunigramdict.keys()\n",
    "    n = 6\n",
    "    for j in range(n):\n",
    "        counter = 0\n",
    "        queryz = []\n",
    "        k = 0\n",
    "        for i in quer1:\n",
    "            if i not in cacmunigramdict.keys():\n",
    "                counter = 1\n",
    "                simwords = getallsimilar(i, diction)\n",
    "                queryz.append(color.BOLD + simwords[j] + color.END)\n",
    "                k = 1\n",
    "            else:\n",
    "                queryz.append(i)\n",
    "        if (k == 1):\n",
    "            print(\"Did you Mean : \",\" \".join(queryz)) \n",
    "            print('\\n')\n",
    "    if (counter == 0):\n",
    "        querycheck = 1\n",
    "    if (querycheck == 1):\n",
    "        ar = BM25(\" \".join(queryz), documentandlen = cacmlen, uniindex = cacmunigramdict)\n",
    "        ar = sorted(ar.items(), key=operator.itemgetter(1), reverse = True)\n",
    "        ar = pd.DataFrame(ar)\n",
    "        for i in range(10):\n",
    "            print(\"\\n\")\n",
    "            print(\"Result:\",i+1)\n",
    "            print(\"Document Name :\", color.BOLD + ar[0][i] + color.END)\n",
    "            print(getsnippet(\" \".join(queryz), diction1, ar[0][i], k = 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
